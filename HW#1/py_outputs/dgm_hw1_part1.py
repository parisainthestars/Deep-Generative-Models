# -*- coding: utf-8 -*-
"""DGM_HW1_part1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f75_l4YyeARRmpyD22pd-hGDUqV1_Gw8
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, random_split
import numpy as np
import matplotlib.pyplot as plt
import os

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

H_DIM = 20

IMAGE_DIM = (64, 64)

EPOCHS = 50
LEARNING_RATE = 0.001
BATCH_SIZE = 128

dataset_url = "https://github.com/deepmind/dsprites-dataset/raw/master/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz"
dataset_path = "dsprites.npz"

if not os.path.exists(dataset_path):
    print("Downloading dsprites dataset...")

    !wget -q -O {dataset_path} {dataset_url}
    print("Download complete.")
else:
    print("Dataset already downloaded.")


class DSpritesDataset(Dataset):
    def __init__(self, npz_path):
        print("Loading dataset with memory mapping (mmap_mode)...")
        data = np.load(npz_path, mmap_mode='r')

        self.images = data['imgs']
        print(f"Dataset loaded. Total samples: {len(self.images)}")

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):

        image_np = self.images[idx]


        image_tensor = torch.from_numpy(image_np.astype(np.float32)).unsqueeze(0)

        return image_tensor


full_dataset = DSpritesDataset(dataset_path)
train_size = int(0.9 * len(full_dataset))
test_size = len(full_dataset) - train_size
train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])


train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

print(f"DataLoaders created. Training samples: {len(train_dataset)}")

class VAE(nn.Module):
    def __init__(self, h_dim):
        super(VAE, self).__init__()


        self.encoder = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Flatten()
        )


        self.fc_mu = nn.Linear(8192, h_dim)
        self.fc_logvar = nn.Linear(8192, h_dim)


        self.decoder_input = nn.Linear(h_dim, 8192)


        self.decoder = nn.Sequential(

            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.Sigmoid()
        )

    def encode(self, x):
        h = self.encoder(x)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):

        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        h = F.relu(self.decoder_input(z))
        h = h.view(-1, 128, 8, 8)
        return self.decoder(h)

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon_x = self.decode(z)
        return recon_x, mu, logvar

def loss_function(recon_x, x, mu, logvar):

    BCE = F.binary_cross_entropy(recon_x.view(-1, 64*64), x.view(-1, 64*64), reduction='sum')


    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

    total_loss = (BCE + KLD) / x.size(0)

    avg_bce = BCE / x.size(0)
    avg_kld = KLD / x.size(0)

    return total_loss, avg_bce, avg_kld

model = VAE(h_dim=H_DIM).to(device)

optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

history_total_loss = []
history_recon_loss = []
history_kl_loss = []

print("Starting training...")

for epoch in range(EPOCHS):
    model.train()

    epoch_total_loss = 0
    epoch_recon_loss = 0
    epoch_kl_loss = 0

    for batch_idx, data in enumerate(train_loader):
        data = data.to(device)

        recon_batch, mu, logvar = model(data)

        loss, recon_loss, kl_loss = loss_function(recon_batch, data, mu, logvar)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_total_loss += loss.item()
        epoch_recon_loss += recon_loss.item()
        epoch_kl_loss += kl_loss.item()

    avg_total_loss = epoch_total_loss / len(train_loader)
    avg_recon_loss = epoch_recon_loss / len(train_loader)
    avg_kl_loss = epoch_kl_loss / len(train_loader)

    history_total_loss.append(avg_total_loss)
    history_recon_loss.append(avg_recon_loss)
    history_kl_loss.append(avg_kl_loss)

    print(f"Epoch [{epoch+1}/{EPOCHS}], "
          f"Total Loss: {avg_total_loss:.4f}, "
          f"Recon Loss: {avg_recon_loss:.4f}, "
          f"KL Loss: {avg_kl_loss:.4f}")

print("Training finished.")

print("\n--- بخش ۷: خروجی‌های نهایی ---")

print("1. Plotting training losses...")
plt.figure(figsize=(10, 6))
plt.plot(history_total_loss, label='Total Loss (BCE + KL)', linewidth=2)
plt.plot(history_recon_loss, label='Reconstruction Loss (BCE)', linestyle='--')
plt.plot(history_kl_loss, label='KL Divergence Loss', linestyle=':')
plt.title('Training Losses (Total, Reconstruction, and KL) per Epoch')
plt.xlabel('Epochs')
plt.ylabel('Average Loss')
plt.legend()
plt.grid(True)
plt.show()

print("2. Displaying 8 random test samples and their reconstructions...")


test_images_batch = next(iter(test_loader)).to(device)
test_images = test_images_batch[:8]

model.eval()

with torch.no_grad():
    recon_images, _, _ = model(test_images)

originals = test_images.cpu().numpy().squeeze()
reconstructions = recon_images.cpu().numpy().squeeze()

fig, axes = plt.subplots(2, 8, figsize=(16, 4))
fig.suptitle('Original vs. Reconstructed Images', fontsize=16)

for i in range(8):

    axes[0, i].imshow(originals[i], cmap='gray')
    axes[0, i].set_title(f"Original {i+1}")
    axes[0, i].axis('off')

    axes[1, i].imshow(reconstructions[i], cmap='gray')
    axes[1, i].set_title(f"Recon {i+1}")
    axes[1, i].axis('off')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

